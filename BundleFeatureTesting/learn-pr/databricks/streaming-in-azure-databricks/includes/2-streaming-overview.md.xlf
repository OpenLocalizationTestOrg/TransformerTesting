<?xml version="1.0"?><xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd"><file datatype="xml" original="2-streaming-overview.md" source-language="en-US" target-language="en-US"><header><tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-1931010" tool-company="Microsoft" /><xliffext:skl_file_name xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">2-streaming-overview.596fe675cd259c2a47ef441a39d7fde3acf4df65.skl</xliffext:skl_file_name><xliffext:version xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1.2</xliffext:version><xliffext:ms.openlocfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">bd1df09fbb574828bd388aa48a5296ed46a7fb51</xliffext:ms.openlocfilehash><xliffext:ms.lasthandoff xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">04/24/2019</xliffext:ms.lasthandoff><xliffext:ms.openlocfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">learn-pr\databricks\streaming-in-azure-databricks\includes\2-streaming-overview.md</xliffext:ms.openlocfilepath></header><body><group id="content" extype="content"><trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Apache Spark Structured Streaming is a fast, scalable, and fault-tolerant stream processing API.</source>
        </trans-unit><trans-unit id="102" translate="yes" xml:space="preserve">
          <source>You can use it to perform analytics on your streaming data in near real time.</source>
        </trans-unit><trans-unit id="103" translate="yes" xml:space="preserve">
          <source>With Structured Streaming, you can use SQL queries to process streaming data in the same way that you would process static data.</source>
        </trans-unit><trans-unit id="104" translate="yes" xml:space="preserve">
          <source>The API continuously increments and updates the final data.</source>
        </trans-unit><trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Event Hubs and Spark Structured Streaming</source>
        </trans-unit><trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Azure Event Hubs is a scalable real-time data ingestion service that processes millions of data in a matter of seconds.</source>
        </trans-unit><trans-unit id="107" translate="yes" xml:space="preserve">
          <source>It can receive large amounts of data from multiple sources and stream the prepared data to Azure Data Lake or Azure Blob storage.</source>
        </trans-unit><trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Azure Event Hubs can be integrated with Spark Structured Streaming to perform processing of messages in near real time.</source>
        </trans-unit><trans-unit id="109" translate="yes" xml:space="preserve">
          <source>You can query and analyze the processed data as it comes by using a Structured Streaming query and Spark SQL.</source>
        </trans-unit><trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Streaming concepts</source>
        </trans-unit><trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Stream processing is where you continuously incorporate new data into Data Lake storage and compute results.</source>
        </trans-unit><trans-unit id="112" translate="yes" xml:space="preserve">
          <source>The streaming data comes in faster than it can be consumed.</source>
        </trans-unit><trans-unit id="113" translate="yes" xml:space="preserve">
          <source>A stream of data is treated as a table to which data is continuously appended.</source>
        </trans-unit><trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Examples of such data include bank card transactions, Internet of Things (IoT) device data, and video game play events.</source>
        </trans-unit><trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Data coming from a stream is typically not ordered in any way.</source>
        </trans-unit><trans-unit id="116" translate="yes" xml:space="preserve">
          <source>A streaming system consists of:</source>
        </trans-unit><trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Input sources such as Kafka, Azure event hubs, files on a distributed system, or TCP-IP sockets</source>
        </trans-unit><trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Sinks such as Kafka, Azure event hubs, various file formats, forEach sinks, console sinks, or memory sinks</source>
        </trans-unit><trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Streaming and Databricks Delta</source>
        </trans-unit><trans-unit id="120" translate="yes" xml:space="preserve">
          <source>In streaming, the problems of traditional data pipelines are exacerbated.</source>
        </trans-unit><trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Specifically, with frequent metadata refreshes, table repairs and small files accumulate every minute or second.</source>
        </trans-unit><trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Many small files result because data might be streamed in at low volumes with short triggers.</source>
        </trans-unit><trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Databricks Delta is uniquely designed to address these needs.</source>
        </trans-unit><trans-unit id="124" translate="yes" xml:space="preserve">
          <source>In this module, we'll explore some of these capabilities and look at how Databricks Delta helps with streaming data.</source>
        </trans-unit></group></body></file></xliff>