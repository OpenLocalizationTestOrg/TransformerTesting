<?xml version="1.0"?><xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd"><file datatype="xml" original="2-advanced-transformation.md" source-language="en-US" target-language="en-US"><header><tool tool-id="mdxliff" tool-name="mdxliff"  tool-company="Microsoft" /><xliffext:skl_file_name xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">2-advanced-transformation.1c08f5.9f840b16ce5cf1fe78a8693e46cf8aa653dbfe70.skl</xliffext:skl_file_name><xliffext:version xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1.2</xliffext:version><xliffext:ms.openlocfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">9f840b16ce5cf1fe78a8693e46cf8aa653dbfe70</xliffext:ms.openlocfilehash><xliffext:ms.sourcegitcommit xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ba7e452ac39d7f09a5646044b44de0e1cdc54982</xliffext:ms.sourcegitcommit><xliffext:ms.openlocfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">learn-pr\databricks\perform-advanced-data-transformation-in-azure-databricks\includes\2-advanced-transformation.md</xliffext:ms.openlocfilepath></header><body><group id="content" extype="content"><trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Advanced extract, transform, load (ETL) processes include data transformation by using custom and advanced user-defined functions (UDFs), managing complex tables, and loading data into multiple databases simultaneously.</source>
        </trans-unit><trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Custom and complex transformations with UDFs</source>
        </trans-unit><trans-unit id="103" translate="yes" xml:space="preserve">
          <source>The highly optimized built-in functions in Spark provide a wide array of functionality, covering most data transformation use cases.</source>
        </trans-unit><trans-unit id="104" translate="yes" xml:space="preserve">
          <source>However, there may be a scenario when you need to define logic that is specific to your use case or when you need to encapsulate that solution for reuse.</source>
        </trans-unit><trans-unit id="105" translate="yes" xml:space="preserve">
          <source>UDFs are helpful in such cases.</source>
        </trans-unit><trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Use UDFs when there's no clear way to accomplish a task by using built-in functions.</source>
        </trans-unit><trans-unit id="107" translate="yes" xml:space="preserve">
          <source>UDFs provide custom, generalizable code that you can apply to ETL workloads when the built-in functions in Spark aren't sufficient.</source>
        </trans-unit><trans-unit id="108" translate="yes" xml:space="preserve">
          <source>UDFs can't return multiple columns.</source>
        </trans-unit><trans-unit id="109" translate="yes" xml:space="preserve">
          <source>However, they take multiple column inputs, and they return complex named types that are easily accessible.</source>
        </trans-unit><trans-unit id="110" translate="yes" xml:space="preserve">
          <source>This approach is especially helpful in ETL workloads that need to clean complex and challenging data structures.</source>
        </trans-unit><trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Comparison between built-in functions and UDFs</source>
        </trans-unit><trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Joins and lookup tables</source>
        </trans-unit><trans-unit id="113" translate="yes" xml:space="preserve">
          <source>A common use case in ETL jobs joins new data to lookup tables or to historical data.</source>
        </trans-unit><trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Traditional databases join tables by pairing values on a specific column.</source>
        </trans-unit><trans-unit id="115" translate="yes" xml:space="preserve">
          <source>A standard (or shuffle) join moves all the data on the cluster for each table to a specific node on the cluster.</source>
        </trans-unit><trans-unit id="116" translate="yes" xml:space="preserve">
          <source>A standard join uses considerable computation to perform row-wise comparisons and to transfer data across a network, which is often the biggest performance bottleneck of distributed systems.</source>
        </trans-unit><trans-unit id="117" translate="yes" xml:space="preserve">
          <source>A broadcast join fixes this situation when one dataframe is sufficiently small.</source>
        </trans-unit><trans-unit id="118" translate="yes" xml:space="preserve">
          <source>A broadcast join duplicates the smaller dataframe on each node of the cluster, which avoids the cost of shuffling the bigger dataframe.</source>
        </trans-unit><trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Standard and broadcast join between tables</source>
        </trans-unit><trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Writing to multiple databases</source>
        </trans-unit><trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Loading your transformed data to multiple target databases can be a time-consuming activity and can have an impact on your database connection.</source>
        </trans-unit><trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Spark makes this job easier because there are several variables that help optimize performance.</source>
        </trans-unit><trans-unit id="123" translate="yes" xml:space="preserve">
          <source>These variables relate to how data is organized on the cluster.</source>
        </trans-unit><trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Use partitions to get better performance from your database connections.</source>
        </trans-unit><trans-unit id="125" translate="yes" xml:space="preserve">
          <source>A partition is a portion of your total data set.</source>
        </trans-unit><trans-unit id="126" translate="yes" xml:space="preserve">
          <source>The data set is divided into many of these portions so that Spark can distribute your work across a cluster.</source>
        </trans-unit><trans-unit id="127" translate="yes" xml:space="preserve">
          <source>The other concept needed to understand computation in Spark is a slot (also known as a core).</source>
        </trans-unit><trans-unit id="128" translate="yes" xml:space="preserve">
          <source>A slot is a resource that is available for the execution of computation in parallel.</source>
        </trans-unit><trans-unit id="129" translate="yes" xml:space="preserve">
          <source>A partition refers to the distribution of data and a slot refers to the distribution of computation.</source>
        </trans-unit><trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Table management</source>
        </trans-unit><trans-unit id="131" translate="yes" xml:space="preserve">
          <source>When loading data back to the target database, you might want to optimize your data storage by using managed and unmanaged tables.</source>
        </trans-unit><trans-unit id="132" translate="yes" xml:space="preserve">
          <source>A managed table is a table that manages both the actual data and the metadata.</source>
        </trans-unit><trans-unit id="133" translate="yes" xml:space="preserve">
          <source>In this case, a <ph id="ph1">`DROP TABLE`</ph> command removes both the metadata for the table and the data itself.</source>
        </trans-unit><trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Unmanaged tables manage the metadata from a table, while the actual data is managed separately and is often backed by a blob store such as Azure Blob storage.</source>
        </trans-unit><trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Dropping an unmanaged table drops only the metadata that is associated with the table while the data remains in place.</source>
        </trans-unit><trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Structure of managed and unmanaged tables in Databricks</source>
        </trans-unit></group></body></file></xliff>