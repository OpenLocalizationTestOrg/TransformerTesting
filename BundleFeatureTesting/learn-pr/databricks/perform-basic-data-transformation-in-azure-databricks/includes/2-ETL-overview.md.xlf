<?xml version="1.0"?><xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd"><file datatype="xml" original="2-ETL-overview.md" source-language="en-US" target-language="en-US"><header><tool tool-id="mdxliff" tool-name="mdxliff"  tool-company="Microsoft" /><xliffext:skl_file_name xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">2-ETL-overview.b2239b.d6e4514ba3bc8da7f0876202a2a6bd0a68fc362c.skl</xliffext:skl_file_name><xliffext:version xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1.2</xliffext:version><xliffext:ms.openlocfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">d6e4514ba3bc8da7f0876202a2a6bd0a68fc362c</xliffext:ms.openlocfilehash><xliffext:ms.sourcegitcommit xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ba7e452ac39d7f09a5646044b44de0e1cdc54982</xliffext:ms.sourcegitcommit><xliffext:ms.openlocfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">learn-pr\databricks\perform-basic-data-transformation-in-azure-databricks\includes\2-ETL-overview.md</xliffext:ms.openlocfilepath></header><body><group id="content" extype="content"><trans-unit id="101" translate="yes" xml:space="preserve">
          <source>The extract, transform, and load (ETL) process takes data from one or more sources, transforms it (normally by adding structure), and then loads it into a target database.</source>
        </trans-unit><trans-unit id="102" translate="yes" xml:space="preserve">
          <source>A common ETL job takes log files from a web server, parses out the pertinent fields so that the data can be readily queried, and then loads the data into a database.</source>
        </trans-unit><trans-unit id="103" translate="yes" xml:space="preserve">
          <source>ETL may seem simple: applying structure to data so that itâ€™s in the desired form.</source>
        </trans-unit><trans-unit id="104" translate="yes" xml:space="preserve">
          <source>But the complexity of ETL is in the details.</source>
        </trans-unit><trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Data engineers who are building ETL pipelines must understand how to:</source>
        </trans-unit><trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Optimize data formats and connections.</source>
        </trans-unit><trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Determine the ideal schema.</source>
        </trans-unit><trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Handle corrupt records.</source>
        </trans-unit><trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Automate workloads.</source>
        </trans-unit><trans-unit id="110" translate="yes" xml:space="preserve">
          <source>An illustration showing an overview of the ETL process.</source>
        </trans-unit><trans-unit id="111" translate="yes" xml:space="preserve">
          <source>The Spark approach</source>
        </trans-unit><trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Spark offers a compute engine and connectors to virtually any data source.</source>
        </trans-unit><trans-unit id="113" translate="yes" xml:space="preserve">
          <source>By using easily scaled infrastructure and accessing data where it lives, Spark addresses the core needs of a big-data application.</source>
        </trans-unit><trans-unit id="114" translate="yes" xml:space="preserve">
          <source>The following principles make up the Spark approach to ETL and provide a unified and scalable approach to big-data pipelines:</source>
        </trans-unit><trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Databricks and Spark offer a unified platform.</source>
        </trans-unit><trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Spark on Databricks combines ETL, stream processing, machine learning, and collaborative notebooks.</source>
        </trans-unit><trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Data scientists, analysts, and engineers can write Spark code in Python, Scala, SQL, and R.</source>
        </trans-unit><trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Spark's unified platform is scalable to petabytes of data and clusters of thousands of nodes.</source>
        </trans-unit><trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The same code written on smaller datasets scales to large workloads, often with only minor changes.</source>
        </trans-unit><trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Spark on Databricks decouples data storage from the compute and query engine.</source>
        </trans-unit><trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Spark's query engine connects to any number of data sources, such as S3, Azure Blob storage, Redshift, and Kafka.</source>
        </trans-unit><trans-unit id="122" translate="yes" xml:space="preserve">
          <source>This approach minimizes costs because there's no need to maintain a dedicated cluster, and you can easily update the compute cluster to the latest version of Spark.</source>
        </trans-unit><trans-unit id="123" translate="yes" xml:space="preserve">
          <source>An illustration showing the various capabilities of Apache Spark.</source>
        </trans-unit><trans-unit id="124" translate="yes" xml:space="preserve">
          <source>ETL process</source>
        </trans-unit><trans-unit id="125" translate="yes" xml:space="preserve">
          <source>A typical ETL process in Databricks includes the following steps:</source>
        </trans-unit><trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Extraction:<ept id="p1">**</ept> By using Java Database Connectivity (JDBC), you can virtually connect to any data store, including Azure Blob storage.</source>
        </trans-unit><trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Databricks supports connections to multiple database types, including:</source>
        </trans-unit><trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Traditional databases, like PostgreSQL, SQL Server, and MySQL.</source>
        </trans-unit><trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Message brokers, like Kafka and Kinesis.</source>
        </trans-unit><trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Distributed databases, like Cassandra and Redshift.</source>
        </trans-unit><trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Data warehouses, like Hive and Azure Cosmos DB.</source>
        </trans-unit><trans-unit id="132" translate="yes" xml:space="preserve">
          <source>File types, like CSV, Parquet, and Avro.</source>
        </trans-unit><trans-unit id="133" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Data validation:<ept id="p1">**</ept> One aspect of ETL jobs is to validate that the data is what you expect.</source>
        </trans-unit><trans-unit id="134" translate="yes" xml:space="preserve">
          <source>The data-validation step primarily includes determining:</source>
        </trans-unit><trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Approximately the expected number of records.</source>
        </trans-unit><trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Whether the expected fields are present.</source>
        </trans-unit><trans-unit id="137" translate="yes" xml:space="preserve">
          <source>That there are no unexpected missing values.</source>
        </trans-unit><trans-unit id="138" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Transformation:<ept id="p1">**</ept> This step generally includes applying structure and a schema to your data so that you can transform it into the desired format.</source>
        </trans-unit><trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Schemas can be applied to tabular data, such as data found in CSV files or relational databases, or to semistructured data, such as JSON data.</source>
        </trans-unit><trans-unit id="140" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Corrupt record handling:<ept id="p1">**</ept> Handling bad data is also an important part of the entire ETL process.</source>
        </trans-unit><trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The built-in functions of Databricks allow you to handle corrupt data, such as:</source>
        </trans-unit><trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Missing and incomplete information.</source>
        </trans-unit><trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Schema mismatches.</source>
        </trans-unit><trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Differing formats or data types.</source>
        </trans-unit><trans-unit id="145" translate="yes" xml:space="preserve">
          <source>User errors when creating data producers.</source>
        </trans-unit><trans-unit id="146" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Loading data:<ept id="p1">**</ept> A common and highly effective design pattern in the Databricks and Spark ecosystem involves loading structured data back to the Databricks File System (DBFS) as a Parquet file.</source>
        </trans-unit><trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Databricks also allows you to put code into production by scheduling notebooks for regular execution.</source>
        </trans-unit><trans-unit id="148" translate="yes" xml:space="preserve">
          <source>In this module, we'll explore some basic ETL techniques by using Azure Databricks.</source>
        </trans-unit></group></body></file></xliff>