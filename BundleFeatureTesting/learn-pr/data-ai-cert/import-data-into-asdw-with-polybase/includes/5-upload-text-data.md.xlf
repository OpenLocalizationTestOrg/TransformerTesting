<?xml version="1.0"?><xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd"><file datatype="xml" original="5-upload-text-data.md" source-language="en-US" target-language="en-US"><header><tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-1931010" tool-company="Microsoft" /><xliffext:skl_file_name xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">5-upload-text-data.b30e121eb952cfe075b5c3345cab41459f45ba16.skl</xliffext:skl_file_name><xliffext:version xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1.2</xliffext:version><xliffext:ms.openlocfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">45c328670c217067b76b54b6fa17649d03598644</xliffext:ms.openlocfilehash><xliffext:ms.lasthandoff xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">05/13/2019</xliffext:ms.lasthandoff><xliffext:ms.openlocfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">learn-pr\data-ai-cert\import-data-into-asdw-with-polybase\includes\5-upload-text-data.md</xliffext:ms.openlocfilepath></header><body><group id="content" extype="content"><trans-unit id="101" translate="yes" xml:space="preserve">
          <source>PolyBase can read data from several file formats and data sources.</source>
        </trans-unit><trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Before you upload your data into Azure SQL Data Warehouse, you must prepare the source data into an acceptable format for PolyBase.</source>
        </trans-unit><trans-unit id="103" translate="yes" xml:space="preserve">
          <source>These formats include:</source>
        </trans-unit><trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Comma-delimited text files (UTF-8 and UTF-16).</source>
        </trans-unit><trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Hadoop file formats, such as RC files, Optimized Row Columnar (ORC) files, and Parquet files.</source>
        </trans-unit><trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Gzip and Snappy compressed files.</source>
        </trans-unit><trans-unit id="107" translate="yes" xml:space="preserve">
          <source>If the data is coming from a relational database, such as Microsoft SQL Server, pull the data and then load it into an acceptable data store, such as an Azure Blob storage account.</source>
        </trans-unit><trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Tools such as SQL Server Integration Services can ease this transfer process.</source>
        </trans-unit><trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Let's take some sample data and upload it to our Blob storage container.</source>
        </trans-unit><trans-unit id="110" translate="yes" xml:space="preserve">
          <source>This exercise is optional.</source>
        </trans-unit><trans-unit id="111" translate="yes" xml:space="preserve">
          <source>If you don't have an Azure account, or prefer not to do the exercise in your account, read the instructions to understand how to upload a text file to Azure Blob storage.</source>
        </trans-unit><trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Obtain the source data</source>
        </trans-unit><trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Our data processing department has exported the data we need into a comma-delimited file.</source>
        </trans-unit><trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Let's import that data into the blob container we created earlier.</source>
        </trans-unit><trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Start by downloading the <bpt id="p1">[</bpt>sample data file<ept id="p1">](https://raw.githubusercontent.com/MicrosoftDocs/mslearn-implement-azure-sql-data-warehouse/master/import-data-into-asdw-with-polybase/DimDate2.txt)</ept> to your local computer.</source>
        </trans-unit><trans-unit id="116" translate="yes" xml:space="preserve">
          <source>The file contains time and data information with details about each entry.</source>
        </trans-unit><trans-unit id="117" translate="yes" xml:space="preserve">
          <source>The text file is named <ph id="ph1">`DimDate2.txt`</ph> and has 1,188 rows, or lines, of data.</source>
        </trans-unit><trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Each line of data is separated into 12-column values by using commas.</source>
        </trans-unit><trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Here's a sample of the data file:</source>
        </trans-unit><trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Import data into blob storage</source>
        </trans-unit><trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Let's upload the data into the blob container.</source>
        </trans-unit><trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Sign in to the <bpt id="p1">[</bpt>Azure portal<ept id="p1">](https://portal.azure.com?azure-portal=true)</ept> with the Azure account you created the storage account in.</source>
        </trans-unit><trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Select <bpt id="p1">**</bpt>All resources<ept id="p1">**</ept>, and in the <bpt id="p2">**</bpt>Search<ept id="p2">**</ept> box, enter <bpt id="p3">**</bpt>demodwstorage<ept id="p3">**</ept>.</source>
        </trans-unit><trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Select your storage account from the results.</source>
        </trans-unit><trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Select <bpt id="p1">**</bpt>Blobs<ept id="p1">**</ept> from the <bpt id="p2">**</bpt>Blob service<ept id="p2">**</ept> section in the storage account.</source>
        </trans-unit><trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Blob container in the Azure portal</source>
        </trans-unit><trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Select the container named <bpt id="p1">**</bpt>data-files<ept id="p1">**</ept> to open it.</source>
        </trans-unit><trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Select the <bpt id="p1">**</bpt>Upload<ept id="p1">**</ept> icon.</source>
        </trans-unit><trans-unit id="129" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p1">**</bpt>Upload blob<ept id="p1">**</ept> blade on the right, browse and select the <bpt id="p2">**</bpt>DimDate2.txt<ept id="p2">**</ept> file you downloaded.</source>
        </trans-unit><trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Uploading data file</source>
        </trans-unit><trans-unit id="131" translate="yes" xml:space="preserve">
          <source>After the file is uploaded, close the <bpt id="p1">**</bpt>Upload blob<ept id="p1">**</ept> blade.</source>
        </trans-unit><trans-unit id="132" translate="yes" xml:space="preserve">
          <source>The file appears in your blob container.</source>
        </trans-unit><trans-unit id="133" translate="yes" xml:space="preserve">
          <source>This example uses a single text file to upload data.</source>
        </trans-unit><trans-unit id="134" translate="yes" xml:space="preserve">
          <source>It's a best practice to split up the data between data files of equal size that match the number of compute nodes in your data warehouse.</source>
        </trans-unit><trans-unit id="135" translate="yes" xml:space="preserve">
          <source>That way you gain full parallelism of all the text files against each available compute node.</source>
        </trans-unit><trans-unit id="136" translate="yes" xml:space="preserve">
          <source>For example, if you use a <bpt id="p1">**</bpt>Gen1 - DWU6000<ept id="p1">**</ept> or <bpt id="p2">**</bpt>Gen2 - DW30000c<ept id="p2">**</ept> configuration, you can import 60 text files in parallel because there are 60 nodes.</source>
        </trans-unit></group></body></file></xliff>