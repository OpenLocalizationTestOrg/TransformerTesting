<?xml version="1.0"?><xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd"><file datatype="xml" original="6-use-cases.md" source-language="en-US" target-language="en-US"><header><tool tool-id="mdxliff" tool-name="mdxliff"  tool-company="Microsoft" /><xliffext:skl_file_name xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">6-use-cases.459216.f05459a54768cbb09c0a4d4fba22d3b7f5221fbe.skl</xliffext:skl_file_name><xliffext:version xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1.2</xliffext:version><xliffext:ms.openlocfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">f05459a54768cbb09c0a4d4fba22d3b7f5221fbe</xliffext:ms.openlocfilehash><xliffext:ms.sourcegitcommit xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ba7e452ac39d7f09a5646044b44de0e1cdc54982</xliffext:ms.sourcegitcommit><xliffext:ms.openlocfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">learn-pr\data-ai-cert\introduction-to-azure-data-lake-storage\includes\6-use-cases.md</xliffext:ms.openlocfilepath></header><body><group id="content" extype="content"><trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Let's examine three use cases for using an Azure Data Lake Store.</source>
        </trans-unit><trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Creating a modern data warehouse</source>
        </trans-unit><trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Imagine you're a Data Engineering consultant for Contoso.</source>
        </trans-unit><trans-unit id="104" translate="yes" xml:space="preserve">
          <source>In the past, they've created an on-premises business intelligence solution that used a Microsoft SQL Server Database Engine, Azure Integration Services, Azure Analysis Services, and SQL Server Reporting Services to provide historical reports.</source>
        </trans-unit><trans-unit id="105" translate="yes" xml:space="preserve">
          <source>They tried using the Analysis Services Data Mining component to create a predictive analytics solution to predict the buying behavior of customers.</source>
        </trans-unit><trans-unit id="106" translate="yes" xml:space="preserve">
          <source>While this approach worked well with low volumes of data, it couldn't scale after more than a gigabyte of data was collected.</source>
        </trans-unit><trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Furthermore, they were never able to deal with the JSON data that a third-party application generated when a customer used the feedback module of the point of sale (POS) application.</source>
        </trans-unit><trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Contoso has turned to you for help with creating an architecture that can scale with the data needs that are required to create a predictive model and to handle the JSON data so that it's integrated into the BI solution.</source>
        </trans-unit><trans-unit id="109" translate="yes" xml:space="preserve">
          <source>You suggest the following architecture:</source>
        </trans-unit><trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Screenshot of a modern data warehouse architecture.</source>
        </trans-unit><trans-unit id="111" translate="yes" xml:space="preserve">
          <source>The architecture uses Azure Data Lake Storage at the center of the solution for a modern data warehouse.</source>
        </trans-unit><trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Integration Services is replaced by Azure Data Factory to ingest data into the Data Lake from a business application.</source>
        </trans-unit><trans-unit id="113" translate="yes" xml:space="preserve">
          <source>This is the source for the predictive model that is built into Azure Databricks.</source>
        </trans-unit><trans-unit id="114" translate="yes" xml:space="preserve">
          <source>PolyBase is used to transfer the historical data into a big data relational format that is held in Azure SQL Data Warehouse, which also stores the results of the trained model from Databricks.</source>
        </trans-unit><trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Azure Analysis Services provides the caching capability for SQL Data Warehouse to service many users and to present the data through Power BI reports.</source>
        </trans-unit><trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Advanced analytics for big data</source>
        </trans-unit><trans-unit id="117" translate="yes" xml:space="preserve">
          <source>In this second use case, Azure Data Lake Storage plays an important role in providing a large-scale data store.</source>
        </trans-unit><trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Your skills are needed by AdventureWorks, which is a global seller of bicycles and cycling components through a chain of resellers and on the internet.</source>
        </trans-unit><trans-unit id="119" translate="yes" xml:space="preserve">
          <source>As their customers browse the product catalog on their websites and add items to their baskets, a recommendation engine that is built into Azure Databricks recommends other products.</source>
        </trans-unit><trans-unit id="120" translate="yes" xml:space="preserve">
          <source>They need to make sure that the results of their recommendation engine can scale globally.</source>
        </trans-unit><trans-unit id="121" translate="yes" xml:space="preserve">
          <source>The recommendations are based on the web log files that are stored on the web servers and transferred to the Azure Databricks model hourly.</source>
        </trans-unit><trans-unit id="122" translate="yes" xml:space="preserve">
          <source>The response time for the recommendation should be less than 1 ms.</source>
        </trans-unit><trans-unit id="123" translate="yes" xml:space="preserve">
          <source>You propose the following architecture:</source>
        </trans-unit><trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Screenshot of an Advanced Analytics with big data architecture.</source>
        </trans-unit><trans-unit id="125" translate="yes" xml:space="preserve">
          <source>In this solution, Azure Data Factory transfers terabytes of web logs from a web server to the Azure Data Lake on an hourly basis.</source>
        </trans-unit><trans-unit id="126" translate="yes" xml:space="preserve">
          <source>This data is provided as features to the predictive model in Azure Databricks, which is then trained and scored.</source>
        </trans-unit><trans-unit id="127" translate="yes" xml:space="preserve">
          <source>The results are distributed globally by using Azure Cosmos DB, which the real-time app (the AdventureWorks website) will use to provide recommendations to customers as they add products to their online baskets.</source>
        </trans-unit><trans-unit id="128" translate="yes" xml:space="preserve">
          <source>To complete this architecture, PolyBase is used against the Data Lake to transfer descriptive data to the SQL Data Warehouse for reporting purposes.</source>
        </trans-unit><trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Azure Analysis Services provides the caching capability for SQL Data Warehouse to service many users and to display the data through Power BI reports.</source>
        </trans-unit><trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Real-time analytical solutions</source>
        </trans-unit><trans-unit id="131" translate="yes" xml:space="preserve">
          <source>To perform real-time analytical solutions, the ingestion phase of the architecture is changed for processing big data solutions.</source>
        </trans-unit><trans-unit id="132" translate="yes" xml:space="preserve">
          <source>In this architecture, note the introduction of Apache Kafka for Azure HDInsight to ingest streaming data from an Internet of Things (IoT) device, although this could be replaced with Azure IoT Hub and Azure Stream Analytics.</source>
        </trans-unit><trans-unit id="133" translate="yes" xml:space="preserve">
          <source>The key point is that the data is persisted in Data Lake Storage Gen2 to service other parts of the solution.</source>
        </trans-unit><trans-unit id="134" translate="yes" xml:space="preserve">
          <source>In this use case, you are a Data Engineer for Trey Research, an organization that is working with a transport company to monitor the fleet of Heavy Goods Vehicles (HGV) that drive around Europe.</source>
        </trans-unit><trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Each HGV is equipped with sensor hardware that will continuously report metric data on the temperature, the speed, and the oil and brake solution levels of an HGV.</source>
        </trans-unit><trans-unit id="136" translate="yes" xml:space="preserve">
          <source>When the engine is turned off, the sensor also outputs a file with summary information about a trip, including the mileage and elevation of a trip.</source>
        </trans-unit><trans-unit id="137" translate="yes" xml:space="preserve">
          <source>A trip is a period in which the HGV engine is turned on and off.</source>
        </trans-unit><trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Both the real-time data and batch data is processed in a machine learning model to predict a maintenance schedule for each of the HGVs.</source>
        </trans-unit><trans-unit id="139" translate="yes" xml:space="preserve">
          <source>This data is made available to the downstream application that third-party garage companies can use if an HGV breaks down anywhere in Europe.</source>
        </trans-unit><trans-unit id="140" translate="yes" xml:space="preserve">
          <source>In addition, historical reports about the HGV should be visually presented to users.</source>
        </trans-unit><trans-unit id="141" translate="yes" xml:space="preserve">
          <source>As a result, the following architecture is proposed:</source>
        </trans-unit><trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Screenshots of processing data</source>
        </trans-unit><trans-unit id="143" translate="yes" xml:space="preserve">
          <source>In this architecture, there are two ingestion streams.</source>
        </trans-unit><trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Azure Data Factory ingests the summary files that are generated when the HGV engine is turned off.</source>
        </trans-unit><trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Apache Kafka provides the real-time ingestion engine for the telemetry data.</source>
        </trans-unit><trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Both data streams are stored in Azure Data Lake Store for use in the future, but they are also passed on to other technologies to meet business needs.</source>
        </trans-unit><trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Both streaming and batch data are provided to the predictive model in Azure Databricks, and the results are published to Azure Cosmos DB to be used by the third-party garages.</source>
        </trans-unit><trans-unit id="148" translate="yes" xml:space="preserve">
          <source>PolyBase transfers data from the Data Lake Store into SQL Data Warehouse where Azure Analysis Services creates the HGV reports by using Power BI.</source>
        </trans-unit></group></body></file></xliff>